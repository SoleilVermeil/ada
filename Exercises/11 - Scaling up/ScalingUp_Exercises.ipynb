{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling up\n",
    "\n",
    "Originally released as part of a homework in ADA2019.\n",
    "\n",
    "## Description\n",
    "\n",
    "[Reddit](https://www.reddit.com/) aka *'the front page of the internet'* is a network of over a million *communities* aka *'subreddits'*, each of which covers a different topic based on people's interests. In other words, it is a *massive* collection of forums (corresponding to the aforementioned communities), where people can share content specific to a given topic or comment on other peopleâ€™s posts.   \n",
    "\n",
    "You are reddit's community manager and want to *appoint new moderators*. Because moderating a specific subreddit isn't a full-time job, you want the chosen moderators to moderate multiple subreddits at the same time. To make this choice effective, the moderators shouldn't have to spend too much time getting to know the community and the prevalent communication style, so it makes sense to let moderators moderate subreddits that are similar in communication style and language. At the same time, it also makes sense to let them moderate subreddits that are similar with respect to the participating users, because this allows moderators to track the behavior of individual users over multiple subreddits. For example, some users might only post offensive content once a month on a given subreddit, and therefore fly under the radar with someone moderating only that subreddit. However, considering all the subreddits these users post to, they might post something offensive every day but on different subreddits. Thus, a moderator in charge of all these subreddits would be able to ban such users much more effectively. In the light of the above description, your task is to find out ways to choose moderators considering both the textual content and the users of a subreddit.\n",
    "\n",
    "### Dataset:\n",
    "The dataset provided to you includes all the posts of the 15 largest subreddits written as of May 2015.\n",
    "\n",
    "Reddit posts (provided to you via a [google drive folder](https://drive.google.com/a/epfl.ch/file/d/19SVHKbUTUPtC9HMmADJcAAIY1Xjq6WFv/view?usp=sharing))\n",
    "```\n",
    "reddit_posts\n",
    " |-- id: id of the post \n",
    " |-- author: user name of the author \n",
    " |-- body: text of the message\n",
    " |-- subreddit: name of the subreddit\n",
    "```\n",
    "\n",
    "Reddit scores (provided to you via a [google drive folder](https://drive.google.com/a/epfl.ch/file/d/1vr4PolJzTXr6ODSe3ucib5EAyp3rjxec/view?usp=sharing))\n",
    "```\n",
    "reddit_scores\n",
    " |-- id: id of the post \n",
    " |-- score: score computed as sum of UP/DOWN votes\n",
    "```\n",
    "\n",
    "*Note: Jaccard similarity between subreddits represented using either the set of top-1000 words or the set of users can be computed locally (on the driver), however, all the other tasks have to be implemented in Spark.*\n",
    "\n",
    "## B1. Getting a sense of the data\n",
    "\n",
    "Start a PySpark instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.252.12.100:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2d669a53910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Stop the current session\n",
    "\n",
    "conf = pyspark.SparkConf().setMaster(\"local[*]\").setAll([\n",
    "    ('spark.executor.memory', '12g'),  # find\n",
    "    ('spark.driver.memory','4g'), # your\n",
    "    ('spark.driver.maxResultSize', '2G') # setup\n",
    "])\n",
    "\n",
    "# create the session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# FIX for Spark 2.x\n",
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "\n",
    "spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and load the data in a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reddit_messages = spark.read.json(\"messages.json.gz\")\n",
    "reddit_scores = spark.read.json(\"score.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1.1. Identify the most active subreddit\n",
    "\n",
    "Print the list of subreddits along with the following information:\n",
    "1. The total number of posts\n",
    "2. The number of users with at least 1 message\n",
    "3. The mean message length\n",
    "\n",
    "*Note: Keep everything in one single dataframe and print the list sorted by number of posts in descending order.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print structure of the data\n",
    "reddit_messages.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7984080 messages.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of messages\n",
    "print(f\"There are {reddit_messages.count()} messages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 828424 users.\n"
     ]
    }
   ],
   "source": [
    "# Get users with at least 1 message\n",
    "users = reddit_messages.select(\"author\").distinct()\n",
    "print(f\"There are {users.count()} users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                body|length|\n",
      "+--------------------+------+\n",
      "|gg this one's ove...|    54|\n",
      "|Nihilum and LG ar...|   212|\n",
      "|Me too. Same hamm...|    33|\n",
      "|well i think new ...|   446|\n",
      "|That's something ...|  1036|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "'average_length' has 1 rows and 1 columns.\n",
      "The average length of a message is 148.42.\n"
     ]
    }
   ],
   "source": [
    "# To show how to use some of the commands, here is a small example of usage\n",
    "reddit_messages.select(\n",
    "    \"body\",\n",
    "    length(\"body\").alias(\"length\")\n",
    ").show(5)\n",
    "\n",
    "# Getting the length for all messages\n",
    "lengths = reddit_messages.select(\n",
    "    length(\"body\").alias(\"length\")\n",
    ")\n",
    "average_length_df = lengths.agg({\"length\": \"avg\"})\n",
    "\n",
    "# NOTE: This is still a dataframe. We print the size to show it:\n",
    "print(f\"'average_length' has {average_length_df.count()} rows and {len(average_length_df.columns)} columns.\")\n",
    "\n",
    "# NOTE: We thus can simply take the element (0, 0)\n",
    "average_length = average_length_df.collect()[0][0]\n",
    "\n",
    "print(f\"The average length of a message is {average_length:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1.2. Identify the largest subreddit\n",
    "\n",
    "Print *two* different lists of subreddits: ordered by (1) the number of posts, and (2) the number of users. For each subreddit, print the name and the corresponding counts.\n",
    "\n",
    "Additionally, (3) plot the mean of message length for each subreddit in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      subreddit|  count|\n",
      "+---------------+-------+\n",
      "|leagueoflegends|1151287|\n",
      "|            nba| 704862|\n",
      "|          funny| 691139|\n",
      "|           pics| 564502|\n",
      "|            nfl| 534345|\n",
      "|         videos| 511492|\n",
      "|           news| 477658|\n",
      "| DestinyTheGame| 471160|\n",
      "|         soccer| 455215|\n",
      "|          DotA2| 445154|\n",
      "|      worldnews| 439417|\n",
      "|  AdviceAnimals| 411902|\n",
      "|         hockey| 389329|\n",
      "|GlobalOffensive| 382017|\n",
      "|         movies| 354601|\n",
      "+---------------+-------+\n",
      "\n",
      "+---------------+------+\n",
      "|      subreddit| users|\n",
      "+---------------+------+\n",
      "|          funny|224077|\n",
      "|           pics|205305|\n",
      "|         videos|157628|\n",
      "|leagueoflegends|119321|\n",
      "|  AdviceAnimals|115815|\n",
      "|      worldnews| 99261|\n",
      "|           news| 98736|\n",
      "|         movies| 92484|\n",
      "|GlobalOffensive| 46686|\n",
      "|            nba| 45034|\n",
      "|         soccer| 41648|\n",
      "|            nfl| 41593|\n",
      "|          DotA2| 41466|\n",
      "| DestinyTheGame| 37008|\n",
      "|         hockey| 25568|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subreddit ordered by number of messages\n",
    "subreddits = reddit_messages.groupBy(\"subreddit\").count().orderBy(desc(\"count\"))\n",
    "subreddits.show()\n",
    "\n",
    "# subreddits ordered by number of users\n",
    "subreddits = reddit_messages.groupBy(\"subreddit\").agg(countDistinct(\"author\").alias(\"users\")).orderBy(desc(\"users\"))\n",
    "subreddits.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1.3. Identify the subreddit with the highest average score\n",
    "\n",
    "Print the list of subreddits sorted by their average content scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      "\n",
      "+---------------+------------------+\n",
      "|      subreddit|        avg(score)|\n",
      "+---------------+------------------+\n",
      "|         videos|12.649445152612358|\n",
      "|           pics|12.216559020162904|\n",
      "|          funny|12.041505399058655|\n",
      "|  AdviceAnimals|11.251695791717447|\n",
      "|         soccer|10.634627593554693|\n",
      "|         movies|  9.82014997137628|\n",
      "|            nfl| 9.048348913155358|\n",
      "|            nba| 9.032795071943161|\n",
      "|           news| 8.673421150697779|\n",
      "|      worldnews|  7.86683719564787|\n",
      "|         hockey| 6.520120515039979|\n",
      "|leagueoflegends| 5.983557531701479|\n",
      "|          DotA2| 4.880537971129092|\n",
      "|GlobalOffensive| 4.351442475073099|\n",
      "| DestinyTheGame|3.0288819084811953|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Structure of the score dataframe\n",
    "reddit_scores.printSchema()\n",
    "\n",
    "# Merging scores with messages\n",
    "reddit_messages_scores = reddit_messages.join(other=reddit_scores, on=\"id\")\n",
    "\n",
    "# Grouping messages by subreddit\n",
    "(\n",
    "    reddit_messages_scores\n",
    "    .groupBy(\"subreddit\") # grouping by subreddits\n",
    "    .agg({\"score\": \"avg\"}) # computing the average score (thus for each subreddit)\n",
    "    .orderBy(desc(\"avg(score)\")) # (visual) ordering by average score\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Moderator assignment based on Subreddit Textual Content\n",
    "\n",
    "Different subreddits follow different communication styles inherent in the topic and the community. Having said that, the goal is to discover similar subreddits by only looking at the *words* present in the posted messages. Once such a list of similar subreddits is identified, an appropriately chosen moderator can then be assigned to all these subreddits.\n",
    "\n",
    "Specifically, the task boils down to computing a similarity score between two subreddits based on the *words* present in their textual content. Your first idea is to use the *Jaccard similarity*, which is defined as the size of the intersection of two sets divided by the size of their union.\n",
    "\n",
    "$Jaccard(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2.1.\n",
    "The first step requires constructing a set representation of each subreddit. The goal is to represent each subreddit as a *set of words* existing in the messages posted on that subreddit. Compute the 50,000 most frequent words across all the provided subreddits. Construct a representation for each subreddit by retaining only the words found in the previously identified set of 50,000 frequent words.\n",
    "\n",
    "Some rules:\n",
    " * Words are defined as tokens matching the regular expression `\\W`\n",
    " * Remove all the stop-words (English language)\n",
    "\n",
    "*Note: You might find the [RegexTokenizer](https://spark.apache.org/docs/2.2.0/ml-features.html#tokenizer) and the [StopWordsRemover](https://spark.apache.org/docs/2.2.0/ml-features.html#stopwordsremover) utilities available in the package pyspark.ml useful for this task as they help you in transforming the features and removing stopwords.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------+---------------+--------------------+\n",
      "|           author|                body|     id|      subreddit|               words|\n",
      "+-----------------+--------------------+-------+---------------+--------------------+\n",
      "|        WyaOfWade|gg this one's ove...|cqug90h|            nba|[gg, this, one, s...|\n",
      "|        BEE_REAL_|Nihilum and LG ar...|cqug90p|GlobalOffensive|[nihilum, and, lg...|\n",
      "|        SlowRolla|Me too. Same hamm...|cqug916|           pics|[me, too, same, h...|\n",
      "|   SenpaiOniichan|well i think new ...|cqug919|leagueoflegends|[well, i, think, ...|\n",
      "|backwoodsofcanada|That's something ...|cqug91n|  AdviceAnimals|[that, s, somethi...|\n",
      "+-----------------+--------------------+-------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+--------------------+-------+---------------+--------------------+--------------------+\n",
      "|           author|                body|     id|      subreddit|               words|              tokens|\n",
      "+-----------------+--------------------+-------+---------------+--------------------+--------------------+\n",
      "|        WyaOfWade|gg this one's ove...|cqug90h|            nba|[gg, this, one, s...|[gg, one, watch, ...|\n",
      "|        BEE_REAL_|Nihilum and LG ar...|cqug90p|GlobalOffensive|[nihilum, and, lg...|[nihilum, lg, sig...|\n",
      "|        SlowRolla|Me too. Same hamm...|cqug916|           pics|[me, too, same, h...|   [hammock, fabric]|\n",
      "|   SenpaiOniichan|well i think new ...|cqug919|leagueoflegends|[well, i, think, ...|[well, think, new...|\n",
      "|backwoodsofcanada|That's something ...|cqug91n|  AdviceAnimals|[that, s, somethi...|[something, hear,...|\n",
      "+-----------------+--------------------+-------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------+\n",
      "|  word|  count|\n",
      "+------+-------+\n",
      "|  like|1018978|\n",
      "|people| 745031|\n",
      "|   get| 691349|\n",
      "|   one| 682820|\n",
      "| think| 575352|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Starting by tokenizing the messages\n",
    "# NOTE: inputCol is the name of an existing col, while outputCol is the name of\n",
    "# a col that will be created (actually added) by the tokenizer\n",
    "tokenized_df = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", pattern=\"\\\\W\").transform(reddit_messages)\n",
    "tokenized_df.show(5)\n",
    "\n",
    "# Removing stopwords\n",
    "# NOTE: Same as before for inputCol and outputCol\n",
    "filtered_df = StopWordsRemover(inputCol=\"words\", outputCol=\"tokens\").transform(tokenized_df)\n",
    "filtered_df.show(5)\n",
    "\n",
    "# Making a list of the 50'000 most common words\n",
    "most_common_words = filtered_df.select(explode(\"tokens\").alias(\"word\")).groupBy(\"word\").count().orderBy(desc(\"count\")).limit(50000)\n",
    "most_common_words.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Aucune connexion nâ€™a pu Ãªtre Ã©tablie car lâ€™ordinateur cible lâ€™a expressÃ©ment refusÃ©e",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Getting the list of all words used per subreddit\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m words_for_each_subreddit \u001b[38;5;241m=\u001b[39m filtered_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubreddit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m words_for_each_subreddit\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m      6\u001b[0m list_of_words_for_each_subreddit \u001b[38;5;241m=\u001b[39m words_for_each_subreddit\u001b[38;5;241m.\u001b[39magg(collect_list(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\functions.py:11848\u001b[0m, in \u001b[0;36mexplode\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m  11806\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[0;32m  11807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplode\u001b[39m(col: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m  11808\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  11809\u001b[0m \u001b[38;5;124;03m    Returns a new row for each element in the given array or map.\u001b[39;00m\n\u001b[0;32m  11810\u001b[0m \u001b[38;5;124;03m    Uses the default column name `col` for elements in the array and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11846\u001b[0m \u001b[38;5;124;03m    +---+-----+\u001b[39;00m\n\u001b[0;32m  11847\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 11848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function_over_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexplode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\functions.py:105\u001b[0m, in \u001b[0;36m_invoke_function_over_columns\u001b[1;34m(name, *cols)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_function_over_columns\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    Invokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[38;5;241m*\u001b[39m(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\functions.py:105\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_function_over_columns\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    Invokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[38;5;241m*\u001b[39m(\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\column.py:63\u001b[0m, in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     61\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39m_jc\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m \u001b[43m_create_column_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m     68\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\column.py:56\u001b[0m, in \u001b[0;36m_create_column_from_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_column_from_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     55\u001b[0m     sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVMView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241m.\u001b[39mcol(name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[0;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Aucune connexion nâ€™a pu Ãªtre Ã©tablie car lâ€™ordinateur cible lâ€™a expressÃ©ment refusÃ©e"
     ]
    }
   ],
   "source": [
    "# Getting the list of all words used per subreddit\n",
    "\n",
    "words_for_each_subreddit = filtered_df.select(\"subreddit\", explode(\"tokens\").alias(\"word\"))\n",
    "words_for_each_subreddit.show()\n",
    "\n",
    "list_of_words_for_each_subreddit = words_for_each_subreddit.agg(collect_list(\"word\").alias(\"words\"))\n",
    "list_of_words_for_each_subreddit.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2.2.\n",
    "* Compute the Jaccard similarity between all the subreddits using the set representation obtained in step **B2.1.**, and plot in a heatmap the similarity values of all the pairs of subreddits.\n",
    "* Analyze this plot and discuss your observations. Do you observe that subreddits corresponding to similar topics possess higher Jaccard similarity?\n",
    "* Provide detailed interpretations of the obtained results. Specifically,\n",
    "    - Explain the limitations of your conclusions, and discuss the potential reasons.\n",
    "    - Explain the potential problems with the Jaccard similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2.3.\n",
    "\n",
    "* Alternatively, compute the 1000 most frequent words for each subreddit, construct its representation as the set of top-1000 words, and print a heatmap with the Jaccard similarity like in step **B2.2.**.\n",
    "* Explain your observations in detail: how and why is this new result different from the one obtained in **B2.2.**?\n",
    "\n",
    "*Note: Use the same rules specified in B2.1: words tokenized with the regex \\W and stop-words removed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Moderator assignment based on Subreddit Users\n",
    "\n",
    "Subreddits can be seen as communities of people interacting about a common topic. As an alternative to the *textual content* based similarity in **B2**, your task here is to validate if similarity between two subreddits can be measured based on their participating users.\n",
    "\n",
    "Of course users are not monothematic, and they interact with multiple subreddits. In this task, we are specifically interested in observing the amount of overlap across different subreddits based on their participating users. Similar to **B2**, the overlap is measured using the *Jaccard similarity*.\n",
    "\n",
    "\n",
    "### B3.1.\n",
    "Construct a set representation of each subreddit as the users that posted at least one time in that subreddit.\n",
    "\n",
    "Some users are very talkative and active across different topics. Print the username of the person that posted in the maximum number of subreddits. *Note that users who posted at least once in a subreddit are considered as participant of that subreddit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3.2.\n",
    "\n",
    "* Compute the Jaccard similarity between all the subreddits using the set representation obtained in step **B3.1.**, and visualise it similar to **B2**.\n",
    "* Analyze this plot, identify highly similar pairs of subreddits, and clearly describe your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4. Language vs. Users similarity\n",
    "    \n",
    "* Visualize the similarity scores based on word (**B2.3.**) and user (**B3**) similarity on the x and y axes respectively for the subreddit `NBA` compared to all the other subreddits. Do some semantically meaningful groups emerge? Provide clear explanataions of your observations.\n",
    "* Furthermore, do you observe differences in similarities between various sports-related subreddits in the dataset? Please provide explanations of the reasons behind these differences, if any!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
